{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 images belonging to 1 classes.\n",
      "Found 38 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_dir = \"/home/ayush/Projects/Facial-Recognition-on-Blink-Doorbell/Data/Name(to_train)\"\n",
    "# rescale=1./255, \n",
    "data_generator = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "\n",
    "train_data = data_generator.flow_from_directory(\n",
    "    directory=data_dir,\n",
    "    target_size=(100, 100),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_data = data_generator.flow_from_directory(\n",
    "    directory=data_dir,\n",
    "    target_size=(100, 100),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (x,y),(z,x) \u001b[39m=\u001b[39m train_data\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 18:05:31.576868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-15 18:05:31.577101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-15 18:05:31.577192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-15 18:05:31.577692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-15 18:05:31.577725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-15 18:05:31.577824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-15 18:05:31.577865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2081 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "model = vgg16.VGG16(weights = 'imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(100,100,3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "def layer_adder(bottom_model, num_classes):\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(1024, activation='relu')(top_model)\n",
    "    top_model = Dense(512, activation='relu')(top_model)\n",
    "    top_model = Dense(num_classes, activation='sigmoid')(top_model)\n",
    "    return top_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "FC_Head = layer_adder(model, num_classes)\n",
    "model = Model(inputs = model.input, outputs = FC_Head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,765,313\n",
      "Trainable params: 1,050,625\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = \"/home/ayush/Projects/Facial-Recognition-on-Blink-Doorbell/Data/Name(to_train)/Swapnil/train\"\n",
    "validaton_data_dir = \"/home/ayush/Projects/Facial-Recognition-on-Blink-Doorbell/Data/Name(to_train)/Swapnil/validation\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 45,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validaton_data_dir,\n",
    "    target_size=(100,100),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_274/140307228.py:27: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.0870\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to detector_weights.h15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: detector_weights.h15/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: detector_weights.h15/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 1s/step - loss: 0.0000e+00 - accuracy: 0.0870 - val_loss: 0.0000e+00 - val_accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"detector_weights.h15\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\",\n",
    "                          min_delta= 0,\n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\",\n",
    "              optimizer = \"Adam\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 161\n",
    "nb_validation_samples = 30\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=epochs,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data= validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('detector_weights.h15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import extract_images,resize_images\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
